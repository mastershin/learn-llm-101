{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple multi-head masked self-attention layer.\n",
    "\n",
    "    Attributes:\n",
    "        key (torch.nn.Linear): Linear layer to compute the key matrix.\n",
    "        query (torch.nn.Linear): Linear layer to compute the query matrix.\n",
    "        value (torch.nn.Linear): Linear layer to compute the value matrix.\n",
    "        attn_drop (torch.nn.Dropout): Dropout layer for attention weights.\n",
    "        resid_drop (torch.nn.Dropout): Dropout layer for output.\n",
    "        proj (torch.nn.Linear): Linear layer for final output projection.\n",
    "        mask (torch.Tensor): Buffer storing the mask to apply in the attention scores to ensure causality.\n",
    "        n_head (int): Number of attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the MultiHeadSelfAttention layer.\n",
    "\n",
    "        Args:\n",
    "            config (object): Configuration object containing attributes n_embd, n_head, attn_pdrop, resid_pdrop, and block_size.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
    "\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        \n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "        \n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "        # Creating a causal mask to mask out future tokens in the sequence, ensuring that predictions for a position\n",
    "        # can depend only on the known outputs at positions before it.\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the multi-head masked self-attention layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, T, C) where B is the batch size, T is the sequence length,\n",
    "                              and C is the embedding dimension.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the same shape as input.\n",
    "        \"\"\"\n",
    "        B, T, C = x.size()\n",
    "        hs = C // self.n_head  # Size of each head\n",
    "\n",
    "        k = self.key(x).view(B, T, self.n_head, hs).transpose(1, 2)\n",
    "        q = self.query(x).view(B, T, self.n_head, hs).transpose(1, 2)\n",
    "        v = self.value(x).view(B, T, self.n_head, hs).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention scores using scaled dot-product attention mechanism\n",
    "        k_t = k.transpose(-2, -1)\n",
    "        d_k = k.size(-1)\n",
    "        att = q @ k_t / math.sqrt(d_k)\n",
    "        print(f\"Attention score shape before mask: {att.shape}\")\n",
    "\n",
    "        # Apply causal mask to prevent attending to future tokens\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        print(f\"Attention score shape after mask: {att.shape}\")\n",
    "\n",
    "        # Apply softmax to convert scores to probabilities\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)  # Apply dropout to attention weights\n",
    "\n",
    "        # Multiply attention weights with value matrix\n",
    "        y = torch.matmul(att, v)\n",
    "        print(f\"Output shape before re-assembling: {y.shape}\")\n",
    "\n",
    "        # Re-assemble all head outputs side by side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # Output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention score shape before mask: torch.Size([1, 4, 5, 5])\n",
      "Attention score shape after mask: torch.Size([1, 4, 5, 5])\n",
      "Output shape before re-assembling: torch.Size([1, 4, 5, 3])\n",
      "=== Showing the input and output ===\n",
      "tensor([[[1., 1., 1., 2., 2., 2., 3., 3., 3., 4., 4., 4.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "tensor([[[3.8200, 3.8200, 3.8200, 3.8200, 3.8200, 3.8200, 3.8200, 3.8200,\n",
      "          3.8200, 3.8200, 3.8200, 3.8200],\n",
      "         [3.7831, 3.7831, 3.7831, 3.7831, 3.7831, 3.7831, 3.7831, 3.7831,\n",
      "          3.7831, 3.7831, 3.7831, 3.7831],\n",
      "         [3.7475, 3.7475, 3.7475, 3.7475, 3.7475, 3.7475, 3.7475, 3.7475,\n",
      "          3.7475, 3.7475, 3.7475, 3.7475],\n",
      "         [3.7130, 3.7130, 3.7130, 3.7130, 3.7130, 3.7130, 3.7130, 3.7130,\n",
      "          3.7130, 3.7130, 3.7130, 3.7130],\n",
      "         [3.6797, 3.6797, 3.6797, 3.6797, 3.6797, 3.6797, 3.6797, 3.6797,\n",
      "          3.6797, 3.6797, 3.6797, 3.6797]]], grad_fn=<ViewBackward0>)\n",
      "Gradients: [161, 13, 294, 0, 37187, 432]\n",
      "Success! ðŸš€ðŸš€ðŸš€\n"
     ]
    }
   ],
   "source": [
    "# Test Attention Mechanism\n",
    "# Define a class for GPT configuration settings\n",
    "class GPTConfig:\n",
    "    vocab_size = 11  # Size of the vocabulary\n",
    "    block_size = 5   # Size of each block of tokens\n",
    "    # n_layer = 1  # Number of layers, not used in this exercise\n",
    "    n_head = 4       # Number of attention heads\n",
    "    n_embd = 12      # Dimensionality of embeddings\n",
    "\n",
    "    attn_pdrop = 0.0  # Dropout probability for the attention mechanism\n",
    "    resid_pdrop = 0.0  # Dropout probability for residual connections\n",
    "\n",
    "\n",
    "# Import necessary modules from PyTorch\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Initialize the multi-head self-attention module using GPTConfig settings\n",
    "attention = MultiHeadSelfAttention(GPTConfig())\n",
    "\n",
    "# Define a tensor with a specific structure to represent input embeddings\n",
    "x = torch.tensor([\n",
    "    [\n",
    "        # Embedding vectors, each with 12 dimensions, for each of the 5 tokens\n",
    "        [1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0],\n",
    "        [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Initialize all model parameters to a constant value of 0.1\n",
    "for weight in attention.parameters():\n",
    "    nn.init.constant_(weight, 0.1)\n",
    "\n",
    "# Optionally set the model to evaluation mode to disable dropout layers during inference\n",
    "# attention.eval()\n",
    "\n",
    "# Execute a forward pass through the attention module\n",
    "y = attention(x)\n",
    "\n",
    "# Ensure that the output shape matches the input shape\n",
    "assert y.shape == x.shape\n",
    "\n",
    "# Output the original input tensor and the result of the forward pass\n",
    "print(\"=== Showing the input and output ===\")\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# Prepare for backpropagation by summing all outputs to create a scalar loss\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "# Check for NaN values in the output, which can indicate issues with the softmax computation\n",
    "if torch.isnan(y).any().item():\n",
    "    raise ValueError(\n",
    "        \"It appears that the output contains NaNs. Perhaps the softmax dimension is incorrect?\"\n",
    "    )\n",
    "\n",
    "# Calculate and print the squares of the gradients for each model parameter\n",
    "gradients = [\n",
    "    int((attention.query.weight.grad**2).sum().item()),\n",
    "    int((attention.query.bias.grad**2).sum().item()),\n",
    "    int((attention.key.weight.grad**2).sum().item()),\n",
    "    int((attention.key.bias.grad**2).sum().item()),\n",
    "    int((attention.value.weight.grad**2).sum().item()),\n",
    "    int((attention.value.bias.grad**2).sum().item()),\n",
    "]\n",
    "\n",
    "print(\"Gradients:\", gradients)\n",
    "\n",
    "# Custom checks for specific gradient values to verify correct implementation\n",
    "if gradients == [161, 13, 294, 0, 37187, 432]:\n",
    "    print(\"Correct\")\n",
    "elif gradients == [1, 0, 2, 0, 38787, 432]:\n",
    "    raise RuntimeError(\n",
    "        \"Error: Did you remember to divide by the square root of d_k?\"\n",
    "    )\n",
    "elif gradients[-1] == 432:\n",
    "    raise RuntimeError(\n",
    "        \"There is an error in your implementation. Please check your code. Did you use -inf as the masked_fill_value?\"\n",
    "    )\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"There is an error in your implementation. Please check your code.\"\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jae-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
