{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install textattack transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0665cca832374fd4a1fa224c954adb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c4e6fe838c481e9d5379c1b962cd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4005, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n",
      "{'loss': 1.3806, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n",
      "{'loss': 1.3916, 'learning_rate': 3e-06, 'epoch': 0.02}\n",
      "{'loss': 1.3777, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.03}\n",
      "{'loss': 1.3973, 'learning_rate': 5e-06, 'epoch': 0.03}\n",
      "{'loss': 1.3314, 'learning_rate': 6e-06, 'epoch': 0.04}\n",
      "{'loss': 1.3093, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.05}\n",
      "{'loss': 1.1905, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.05}\n",
      "{'loss': 1.1729, 'learning_rate': 9e-06, 'epoch': 0.06}\n",
      "{'loss': 1.1228, 'learning_rate': 1e-05, 'epoch': 0.07}\n",
      "{'loss': 1.0261, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 0.9813, 'learning_rate': 1.2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.8481, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.09}\n",
      "{'loss': 0.758, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6811, 'learning_rate': 1.5e-05, 'epoch': 0.1}\n",
      "{'loss': 0.6599, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5704, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5879, 'learning_rate': 1.8e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5255, 'learning_rate': 1.9e-05, 'epoch': 0.13}\n",
      "{'loss': 0.439, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4877, 'learning_rate': 2.1e-05, 'epoch': 0.14}\n",
      "{'loss': 0.5408, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4299, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.15}\n",
      "{'loss': 0.6307, 'learning_rate': 2.4e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4067, 'learning_rate': 2.5e-05, 'epoch': 0.17}\n",
      "{'loss': 0.5433, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4082, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.18}\n",
      "{'loss': 0.5822, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.19}\n",
      "{'loss': 0.467, 'learning_rate': 2.9e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4989, 'learning_rate': 3e-05, 'epoch': 0.2}\n",
      "{'loss': 0.44, 'learning_rate': 3.1e-05, 'epoch': 0.21}\n",
      "{'loss': 0.5625, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4931, 'learning_rate': 3.3e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3593, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3484, 'learning_rate': 3.5e-05, 'epoch': 0.23}\n",
      "{'loss': 0.1544, 'learning_rate': 3.6e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3203, 'learning_rate': 3.7e-05, 'epoch': 0.25}\n",
      "{'loss': 0.455, 'learning_rate': 3.8e-05, 'epoch': 0.25}\n",
      "{'loss': 0.7373, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4339, 'learning_rate': 4e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4382, 'learning_rate': 4.1e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3214, 'learning_rate': 4.2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.2825, 'learning_rate': 4.3e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3142, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.29}\n",
      "{'loss': 0.5055, 'learning_rate': 4.5e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3785, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3304, 'learning_rate': 4.7e-05, 'epoch': 0.31}\n",
      "{'loss': 0.5349, 'learning_rate': 4.8e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3646, 'learning_rate': 4.9e-05, 'epoch': 0.33}\n",
      "{'loss': 0.5853, 'learning_rate': 5e-05, 'epoch': 0.33}\n",
      "{'loss': 0.4225, 'learning_rate': 4.9875000000000006e-05, 'epoch': 0.34}\n",
      "{'loss': 0.2627, 'learning_rate': 4.975e-05, 'epoch': 0.35}\n",
      "{'loss': 0.4629, 'learning_rate': 4.962500000000001e-05, 'epoch': 0.35}\n",
      "{'loss': 0.1759, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.36}\n",
      "{'loss': 0.2681, 'learning_rate': 4.937500000000001e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3477, 'learning_rate': 4.9250000000000004e-05, 'epoch': 0.37}\n",
      "{'loss': 0.531, 'learning_rate': 4.9125e-05, 'epoch': 0.38}\n",
      "{'loss': 0.5691, 'learning_rate': 4.9e-05, 'epoch': 0.39}\n",
      "{'loss': 0.4121, 'learning_rate': 4.8875e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3634, 'learning_rate': 4.875e-05, 'epoch': 0.4}\n",
      "{'loss': 0.5978, 'learning_rate': 4.8625e-05, 'epoch': 0.41}\n",
      "{'loss': 0.535, 'learning_rate': 4.85e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3924, 'learning_rate': 4.8375000000000004e-05, 'epoch': 0.42}\n",
      "{'loss': 0.5087, 'learning_rate': 4.825e-05, 'epoch': 0.43}\n",
      "{'loss': 0.2885, 'learning_rate': 4.8125000000000004e-05, 'epoch': 0.43}\n",
      "{'loss': 0.4814, 'learning_rate': 4.8e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3998, 'learning_rate': 4.7875000000000005e-05, 'epoch': 0.45}\n",
      "{'loss': 0.5417, 'learning_rate': 4.775e-05, 'epoch': 0.45}\n",
      "{'loss': 0.355, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.46}\n",
      "{'loss': 0.5145, 'learning_rate': 4.75e-05, 'epoch': 0.47}\n",
      "{'loss': 0.2677, 'learning_rate': 4.7375e-05, 'epoch': 0.47}\n",
      "{'loss': 0.597, 'learning_rate': 4.7249999999999997e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2842, 'learning_rate': 4.7125e-05, 'epoch': 0.49}\n",
      "{'loss': 0.4009, 'learning_rate': 4.7e-05, 'epoch': 0.49}\n",
      "{'loss': 0.4676, 'learning_rate': 4.6875e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5642, 'learning_rate': 4.6750000000000005e-05, 'epoch': 0.51}\n",
      "{'loss': 0.2828, 'learning_rate': 4.6625e-05, 'epoch': 0.51}\n",
      "{'loss': 0.4892, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.52}\n",
      "{'loss': 0.5757, 'learning_rate': 4.6375e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3398, 'learning_rate': 4.6250000000000006e-05, 'epoch': 0.53}\n",
      "{'loss': 0.6292, 'learning_rate': 4.6125e-05, 'epoch': 0.54}\n",
      "{'loss': 0.5622, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3955, 'learning_rate': 4.5875000000000004e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5588, 'learning_rate': 4.575e-05, 'epoch': 0.56}\n",
      "{'loss': 0.4047, 'learning_rate': 4.5625e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3055, 'learning_rate': 4.55e-05, 'epoch': 0.57}\n",
      "{'loss': 0.4128, 'learning_rate': 4.5375e-05, 'epoch': 0.58}\n",
      "{'loss': 0.4797, 'learning_rate': 4.525e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3311, 'learning_rate': 4.5125e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3204, 'learning_rate': 4.5e-05, 'epoch': 0.6}\n",
      "{'loss': 0.4445, 'learning_rate': 4.4875e-05, 'epoch': 0.61}\n",
      "{'loss': 0.1418, 'learning_rate': 4.4750000000000004e-05, 'epoch': 0.61}\n",
      "{'loss': 0.3056, 'learning_rate': 4.4625e-05, 'epoch': 0.62}\n",
      "{'loss': 0.3474, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.63}\n",
      "{'loss': 0.4165, 'learning_rate': 4.4375e-05, 'epoch': 0.63}\n",
      "{'loss': 0.2431, 'learning_rate': 4.4250000000000005e-05, 'epoch': 0.64}\n",
      "{'loss': 0.5701, 'learning_rate': 4.4125e-05, 'epoch': 0.65}\n",
      "{'loss': 0.4583, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.65}\n",
      "{'loss': 0.8103, 'learning_rate': 4.3875e-05, 'epoch': 0.66}\n",
      "{'loss': 0.3546, 'learning_rate': 4.375e-05, 'epoch': 0.67}\n",
      "{'loss': 0.3728, 'learning_rate': 4.3625e-05, 'epoch': 0.67}\n",
      "{'loss': 0.2894, 'learning_rate': 4.35e-05, 'epoch': 0.68}\n",
      "{'loss': 0.3055, 'learning_rate': 4.3375000000000004e-05, 'epoch': 0.69}\n",
      "{'loss': 0.3571, 'learning_rate': 4.325e-05, 'epoch': 0.69}\n",
      "{'loss': 0.4867, 'learning_rate': 4.3125000000000005e-05, 'epoch': 0.7}\n",
      "{'loss': 0.4795, 'learning_rate': 4.3e-05, 'epoch': 0.71}\n",
      "{'loss': 0.1982, 'learning_rate': 4.2875000000000005e-05, 'epoch': 0.71}\n",
      "{'loss': 0.296, 'learning_rate': 4.275e-05, 'epoch': 0.72}\n",
      "{'loss': 0.3716, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.73}\n",
      "{'loss': 0.4037, 'learning_rate': 4.25e-05, 'epoch': 0.73}\n",
      "{'loss': 0.2111, 'learning_rate': 4.237500000000001e-05, 'epoch': 0.74}\n",
      "{'loss': 0.259, 'learning_rate': 4.2250000000000004e-05, 'epoch': 0.75}\n",
      "{'loss': 0.3422, 'learning_rate': 4.2125e-05, 'epoch': 0.75}\n",
      "{'loss': 0.3479, 'learning_rate': 4.2e-05, 'epoch': 0.76}\n",
      "{'loss': 0.4172, 'learning_rate': 4.1875e-05, 'epoch': 0.77}\n",
      "{'loss': 0.432, 'learning_rate': 4.175e-05, 'epoch': 0.77}\n",
      "{'loss': 0.3219, 'learning_rate': 4.1625e-05, 'epoch': 0.78}\n",
      "{'loss': 0.3585, 'learning_rate': 4.15e-05, 'epoch': 0.79}\n",
      "{'loss': 0.2285, 'learning_rate': 4.1375e-05, 'epoch': 0.79}\n",
      "{'loss': 0.2156, 'learning_rate': 4.125e-05, 'epoch': 0.8}\n",
      "{'loss': 0.3538, 'learning_rate': 4.1125000000000004e-05, 'epoch': 0.81}\n",
      "{'loss': 0.3008, 'learning_rate': 4.1e-05, 'epoch': 0.81}\n",
      "{'loss': 0.3086, 'learning_rate': 4.0875000000000004e-05, 'epoch': 0.82}\n",
      "{'loss': 0.2388, 'learning_rate': 4.075e-05, 'epoch': 0.83}\n",
      "{'loss': 0.3733, 'learning_rate': 4.0625000000000005e-05, 'epoch': 0.83}\n",
      "{'loss': 0.3725, 'learning_rate': 4.05e-05, 'epoch': 0.84}\n",
      "{'loss': 0.4495, 'learning_rate': 4.0375e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5726, 'learning_rate': 4.025e-05, 'epoch': 0.85}\n",
      "{'loss': 0.2684, 'learning_rate': 4.0125e-05, 'epoch': 0.86}\n",
      "{'loss': 0.2874, 'learning_rate': 4e-05, 'epoch': 0.87}\n",
      "{'loss': 0.3487, 'learning_rate': 3.9875e-05, 'epoch': 0.87}\n",
      "{'loss': 0.412, 'learning_rate': 3.9750000000000004e-05, 'epoch': 0.88}\n",
      "{'loss': 0.3607, 'learning_rate': 3.9625e-05, 'epoch': 0.89}\n",
      "{'loss': 0.2303, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.89}\n",
      "{'loss': 0.4018, 'learning_rate': 3.9375e-05, 'epoch': 0.9}\n",
      "{'loss': 0.1858, 'learning_rate': 3.9250000000000005e-05, 'epoch': 0.91}\n",
      "{'loss': 0.3684, 'learning_rate': 3.9125e-05, 'epoch': 0.91}\n",
      "{'loss': 0.3895, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.92}\n",
      "{'loss': 0.3494, 'learning_rate': 3.8875e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6305, 'learning_rate': 3.875e-05, 'epoch': 0.93}\n",
      "{'loss': 0.1293, 'learning_rate': 3.8625e-05, 'epoch': 0.94}\n",
      "{'loss': 0.3374, 'learning_rate': 3.85e-05, 'epoch': 0.95}\n",
      "{'loss': 0.2493, 'learning_rate': 3.8375e-05, 'epoch': 0.95}\n",
      "{'loss': 0.1581, 'learning_rate': 3.825e-05, 'epoch': 0.96}\n",
      "{'loss': 0.4844, 'learning_rate': 3.8125e-05, 'epoch': 0.97}\n",
      "{'loss': 0.5721, 'learning_rate': 3.8e-05, 'epoch': 0.97}\n",
      "{'loss': 0.2681, 'learning_rate': 3.7875e-05, 'epoch': 0.98}\n",
      "{'loss': 0.4706, 'learning_rate': 3.775e-05, 'epoch': 0.99}\n",
      "{'loss': 0.3446, 'learning_rate': 3.7625e-05, 'epoch': 0.99}\n",
      "{'loss': 0.4194, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.0}\n",
      "{'loss': 0.253, 'learning_rate': 3.737500000000001e-05, 'epoch': 1.01}\n",
      "{'loss': 0.1192, 'learning_rate': 3.7250000000000004e-05, 'epoch': 1.01}\n",
      "{'loss': 0.068, 'learning_rate': 3.7125e-05, 'epoch': 1.02}\n",
      "{'loss': 0.1381, 'learning_rate': 3.7e-05, 'epoch': 1.03}\n",
      "{'loss': 0.4486, 'learning_rate': 3.6875e-05, 'epoch': 1.03}\n",
      "{'loss': 0.3366, 'learning_rate': 3.675e-05, 'epoch': 1.04}\n",
      "{'loss': 0.1274, 'learning_rate': 3.6625e-05, 'epoch': 1.05}\n",
      "{'loss': 0.3067, 'learning_rate': 3.65e-05, 'epoch': 1.05}\n",
      "{'loss': 0.2535, 'learning_rate': 3.6375e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0659, 'learning_rate': 3.625e-05, 'epoch': 1.07}\n",
      "{'loss': 0.3535, 'learning_rate': 3.6125000000000004e-05, 'epoch': 1.07}\n",
      "{'loss': 0.3365, 'learning_rate': 3.6e-05, 'epoch': 1.08}\n",
      "{'loss': 0.3281, 'learning_rate': 3.5875000000000005e-05, 'epoch': 1.09}\n",
      "{'loss': 0.163, 'learning_rate': 3.575e-05, 'epoch': 1.09}\n",
      "{'loss': 0.1489, 'learning_rate': 3.5625000000000005e-05, 'epoch': 1.1}\n",
      "{'loss': 0.1385, 'learning_rate': 3.55e-05, 'epoch': 1.11}\n",
      "{'loss': 0.2041, 'learning_rate': 3.5375e-05, 'epoch': 1.11}\n",
      "{'loss': 0.2658, 'learning_rate': 3.525e-05, 'epoch': 1.12}\n",
      "{'loss': 0.281, 'learning_rate': 3.5125e-05, 'epoch': 1.13}\n",
      "{'loss': 0.371, 'learning_rate': 3.5e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0854, 'learning_rate': 3.4875e-05, 'epoch': 1.14}\n",
      "{'loss': 0.3526, 'learning_rate': 3.475e-05, 'epoch': 1.15}\n",
      "{'loss': 0.2206, 'learning_rate': 3.4625e-05, 'epoch': 1.15}\n",
      "{'loss': 0.1847, 'learning_rate': 3.45e-05, 'epoch': 1.16}\n",
      "{'loss': 0.2469, 'learning_rate': 3.4375e-05, 'epoch': 1.17}\n",
      "{'loss': 0.1233, 'learning_rate': 3.4250000000000006e-05, 'epoch': 1.17}\n",
      "{'loss': 0.145, 'learning_rate': 3.4125e-05, 'epoch': 1.18}\n",
      "{'loss': 0.2546, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0105, 'learning_rate': 3.3875000000000003e-05, 'epoch': 1.19}\n",
      "{'loss': 0.5591, 'learning_rate': 3.375000000000001e-05, 'epoch': 1.2}\n",
      "{'loss': 0.2351, 'learning_rate': 3.3625000000000004e-05, 'epoch': 1.21}\n",
      "{'loss': 0.3122, 'learning_rate': 3.35e-05, 'epoch': 1.21}\n",
      "{'loss': 0.2082, 'learning_rate': 3.3375e-05, 'epoch': 1.22}\n",
      "{'loss': 0.2448, 'learning_rate': 3.325e-05, 'epoch': 1.23}\n",
      "{'loss': 0.2343, 'learning_rate': 3.3125e-05, 'epoch': 1.23}\n",
      "{'loss': 0.096, 'learning_rate': 3.3e-05, 'epoch': 1.24}\n",
      "{'loss': 0.208, 'learning_rate': 3.2875e-05, 'epoch': 1.25}\n",
      "{'loss': 0.2991, 'learning_rate': 3.275e-05, 'epoch': 1.25}\n",
      "{'loss': 0.1335, 'learning_rate': 3.2625e-05, 'epoch': 1.26}\n",
      "{'loss': 0.2003, 'learning_rate': 3.2500000000000004e-05, 'epoch': 1.27}\n",
      "{'loss': 0.3625, 'learning_rate': 3.2375e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0352, 'learning_rate': 3.2250000000000005e-05, 'epoch': 1.28}\n",
      "{'loss': 0.2231, 'learning_rate': 3.2125e-05, 'epoch': 1.29}\n",
      "{'loss': 0.2417, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.29}\n",
      "{'loss': 0.1282, 'learning_rate': 3.1875e-05, 'epoch': 1.3}\n",
      "{'loss': 0.1772, 'learning_rate': 3.175e-05, 'epoch': 1.31}\n",
      "{'loss': 0.1943, 'learning_rate': 3.1624999999999996e-05, 'epoch': 1.31}\n",
      "{'loss': 0.1273, 'learning_rate': 3.15e-05, 'epoch': 1.32}\n",
      "{'loss': 0.2384, 'learning_rate': 3.1375e-05, 'epoch': 1.33}\n",
      "{'loss': 0.3119, 'learning_rate': 3.125e-05, 'epoch': 1.33}\n",
      "{'loss': 0.5196, 'learning_rate': 3.1125000000000004e-05, 'epoch': 1.34}\n",
      "{'loss': 0.2559, 'learning_rate': 3.1e-05, 'epoch': 1.35}\n",
      "{'loss': 0.1285, 'learning_rate': 3.0875000000000005e-05, 'epoch': 1.35}\n",
      "{'loss': 0.3618, 'learning_rate': 3.075e-05, 'epoch': 1.36}\n",
      "{'loss': 0.4452, 'learning_rate': 3.0625000000000006e-05, 'epoch': 1.37}\n",
      "{'loss': 0.281, 'learning_rate': 3.05e-05, 'epoch': 1.37}\n",
      "{'loss': 0.1886, 'learning_rate': 3.0375000000000003e-05, 'epoch': 1.38}\n",
      "{'loss': 0.059, 'learning_rate': 3.025e-05, 'epoch': 1.39}\n",
      "{'loss': 0.2235, 'learning_rate': 3.0125000000000004e-05, 'epoch': 1.39}\n",
      "{'loss': 0.1854, 'learning_rate': 3e-05, 'epoch': 1.4}\n",
      "{'loss': 0.1697, 'learning_rate': 2.9875000000000004e-05, 'epoch': 1.41}\n",
      "{'loss': 0.3827, 'learning_rate': 2.975e-05, 'epoch': 1.41}\n",
      "{'loss': 0.3526, 'learning_rate': 2.9625000000000002e-05, 'epoch': 1.42}\n",
      "{'loss': 0.2854, 'learning_rate': 2.95e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0793, 'learning_rate': 2.9375000000000003e-05, 'epoch': 1.43}\n",
      "{'loss': 0.114, 'learning_rate': 2.925e-05, 'epoch': 1.44}\n",
      "{'loss': 0.1838, 'learning_rate': 2.9125000000000003e-05, 'epoch': 1.45}\n",
      "{'loss': 0.2324, 'learning_rate': 2.9e-05, 'epoch': 1.45}\n",
      "{'loss': 0.12, 'learning_rate': 2.8875e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0895, 'learning_rate': 2.8749999999999997e-05, 'epoch': 1.47}\n",
      "{'loss': 0.088, 'learning_rate': 2.8625e-05, 'epoch': 1.47}\n",
      "{'loss': 0.1521, 'learning_rate': 2.8499999999999998e-05, 'epoch': 1.48}\n",
      "{'loss': 0.1486, 'learning_rate': 2.8375000000000002e-05, 'epoch': 1.49}\n",
      "{'loss': 0.2231, 'learning_rate': 2.825e-05, 'epoch': 1.49}\n",
      "{'loss': 0.2661, 'learning_rate': 2.8125000000000003e-05, 'epoch': 1.5}\n",
      "{'loss': 0.409, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.51}\n",
      "{'loss': 0.2914, 'learning_rate': 2.7875e-05, 'epoch': 1.51}\n",
      "{'loss': 0.1521, 'learning_rate': 2.7750000000000004e-05, 'epoch': 1.52}\n",
      "{'loss': 0.2114, 'learning_rate': 2.7625e-05, 'epoch': 1.53}\n",
      "{'loss': 0.2273, 'learning_rate': 2.7500000000000004e-05, 'epoch': 1.53}\n",
      "{'loss': 0.2281, 'learning_rate': 2.7375e-05, 'epoch': 1.54}\n",
      "{'loss': 0.3043, 'learning_rate': 2.725e-05, 'epoch': 1.55}\n",
      "{'loss': 0.2893, 'learning_rate': 2.7125000000000002e-05, 'epoch': 1.55}\n",
      "{'loss': 0.3644, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.56}\n",
      "{'loss': 0.393, 'learning_rate': 2.6875e-05, 'epoch': 1.57}\n",
      "{'loss': 0.3418, 'learning_rate': 2.6750000000000003e-05, 'epoch': 1.57}\n",
      "{'loss': 0.2761, 'learning_rate': 2.6625e-05, 'epoch': 1.58}\n",
      "{'loss': 0.1281, 'learning_rate': 2.6500000000000004e-05, 'epoch': 1.59}\n",
      "{'loss': 0.1886, 'learning_rate': 2.6375e-05, 'epoch': 1.59}\n",
      "{'loss': 0.1048, 'learning_rate': 2.625e-05, 'epoch': 1.6}\n",
      "{'loss': 0.1698, 'learning_rate': 2.6124999999999998e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0811, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.61}\n",
      "{'loss': 0.3232, 'learning_rate': 2.5875e-05, 'epoch': 1.62}\n",
      "{'loss': 0.2206, 'learning_rate': 2.5750000000000002e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0864, 'learning_rate': 2.5625e-05, 'epoch': 1.63}\n",
      "{'loss': 0.2702, 'learning_rate': 2.5500000000000003e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0429, 'learning_rate': 2.5375e-05, 'epoch': 1.65}\n",
      "{'loss': 0.3015, 'learning_rate': 2.525e-05, 'epoch': 1.65}\n",
      "{'loss': 0.2424, 'learning_rate': 2.5124999999999997e-05, 'epoch': 1.66}\n",
      "{'loss': 0.2276, 'learning_rate': 2.5e-05, 'epoch': 1.67}\n",
      "{'loss': 0.1793, 'learning_rate': 2.4875e-05, 'epoch': 1.67}\n",
      "{'loss': 0.2085, 'learning_rate': 2.4750000000000002e-05, 'epoch': 1.68}\n",
      "{'loss': 0.199, 'learning_rate': 2.4625000000000002e-05, 'epoch': 1.69}\n",
      "{'loss': 0.1801, 'learning_rate': 2.45e-05, 'epoch': 1.69}\n",
      "{'loss': 0.122, 'learning_rate': 2.4375e-05, 'epoch': 1.7}\n",
      "{'loss': 0.1832, 'learning_rate': 2.425e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0496, 'learning_rate': 2.4125e-05, 'epoch': 1.71}\n",
      "{'loss': 0.2978, 'learning_rate': 2.4e-05, 'epoch': 1.72}\n",
      "{'loss': 0.279, 'learning_rate': 2.3875e-05, 'epoch': 1.73}\n",
      "{'loss': 0.1478, 'learning_rate': 2.375e-05, 'epoch': 1.73}\n",
      "{'loss': 0.1313, 'learning_rate': 2.3624999999999998e-05, 'epoch': 1.74}\n",
      "{'loss': 0.2974, 'learning_rate': 2.35e-05, 'epoch': 1.75}\n",
      "{'loss': 0.1259, 'learning_rate': 2.3375000000000002e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0279, 'learning_rate': 2.3250000000000003e-05, 'epoch': 1.76}\n",
      "{'loss': 0.2107, 'learning_rate': 2.3125000000000003e-05, 'epoch': 1.77}\n",
      "{'loss': 0.1031, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.77}\n",
      "{'loss': 0.342, 'learning_rate': 2.2875e-05, 'epoch': 1.78}\n",
      "{'loss': 0.2607, 'learning_rate': 2.275e-05, 'epoch': 1.79}\n",
      "{'loss': 0.3222, 'learning_rate': 2.2625e-05, 'epoch': 1.79}\n",
      "{'loss': 0.1404, 'learning_rate': 2.25e-05, 'epoch': 1.8}\n",
      "{'loss': 0.3054, 'learning_rate': 2.2375000000000002e-05, 'epoch': 1.81}\n",
      "{'loss': 0.049, 'learning_rate': 2.2250000000000002e-05, 'epoch': 1.81}\n",
      "{'loss': 0.1032, 'learning_rate': 2.2125000000000002e-05, 'epoch': 1.82}\n",
      "{'loss': 0.2214, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.83}\n",
      "{'loss': 0.4772, 'learning_rate': 2.1875e-05, 'epoch': 1.83}\n",
      "{'loss': 0.0517, 'learning_rate': 2.175e-05, 'epoch': 1.84}\n",
      "{'loss': 0.3262, 'learning_rate': 2.1625e-05, 'epoch': 1.85}\n",
      "{'loss': 0.2299, 'learning_rate': 2.15e-05, 'epoch': 1.85}\n",
      "{'loss': 0.1276, 'learning_rate': 2.1375e-05, 'epoch': 1.86}\n",
      "{'loss': 0.2017, 'learning_rate': 2.125e-05, 'epoch': 1.87}\n",
      "{'loss': 0.1013, 'learning_rate': 2.1125000000000002e-05, 'epoch': 1.87}\n",
      "{'loss': 0.452, 'learning_rate': 2.1e-05, 'epoch': 1.88}\n",
      "{'loss': 0.1429, 'learning_rate': 2.0875e-05, 'epoch': 1.89}\n",
      "{'loss': 0.1736, 'learning_rate': 2.075e-05, 'epoch': 1.89}\n",
      "{'loss': 0.1661, 'learning_rate': 2.0625e-05, 'epoch': 1.9}\n",
      "{'loss': 0.2025, 'learning_rate': 2.05e-05, 'epoch': 1.91}\n",
      "{'loss': 0.1444, 'learning_rate': 2.0375e-05, 'epoch': 1.91}\n",
      "{'loss': 0.1385, 'learning_rate': 2.025e-05, 'epoch': 1.92}\n",
      "{'loss': 0.5289, 'learning_rate': 2.0125e-05, 'epoch': 1.93}\n",
      "{'loss': 0.1483, 'learning_rate': 2e-05, 'epoch': 1.93}\n",
      "{'loss': 0.1162, 'learning_rate': 1.9875000000000002e-05, 'epoch': 1.94}\n",
      "{'loss': 0.3123, 'learning_rate': 1.9750000000000002e-05, 'epoch': 1.95}\n",
      "{'loss': 0.1162, 'learning_rate': 1.9625000000000003e-05, 'epoch': 1.95}\n",
      "{'loss': 0.3214, 'learning_rate': 1.9500000000000003e-05, 'epoch': 1.96}\n",
      "{'loss': 0.2942, 'learning_rate': 1.9375e-05, 'epoch': 1.97}\n",
      "{'loss': 0.1857, 'learning_rate': 1.925e-05, 'epoch': 1.97}\n",
      "{'loss': 0.0864, 'learning_rate': 1.9125e-05, 'epoch': 1.98}\n",
      "{'loss': 0.1868, 'learning_rate': 1.9e-05, 'epoch': 1.99}\n",
      "{'loss': 0.3586, 'learning_rate': 1.8875e-05, 'epoch': 1.99}\n",
      "{'loss': 0.2178, 'learning_rate': 1.8750000000000002e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0645, 'learning_rate': 1.8625000000000002e-05, 'epoch': 2.01}\n",
      "{'loss': 0.0038, 'learning_rate': 1.85e-05, 'epoch': 2.01}\n",
      "{'loss': 0.1112, 'learning_rate': 1.8375e-05, 'epoch': 2.02}\n",
      "{'loss': 0.2671, 'learning_rate': 1.825e-05, 'epoch': 2.03}\n",
      "{'loss': 0.258, 'learning_rate': 1.8125e-05, 'epoch': 2.03}\n",
      "{'loss': 0.1954, 'learning_rate': 1.8e-05, 'epoch': 2.04}\n",
      "{'loss': 0.3179, 'learning_rate': 1.7875e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0684, 'learning_rate': 1.775e-05, 'epoch': 2.05}\n",
      "{'loss': 0.1612, 'learning_rate': 1.7625e-05, 'epoch': 2.06}\n",
      "{'loss': 0.059, 'learning_rate': 1.75e-05, 'epoch': 2.07}\n",
      "{'loss': 0.0569, 'learning_rate': 1.7375e-05, 'epoch': 2.07}\n",
      "{'loss': 0.1223, 'learning_rate': 1.725e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0861, 'learning_rate': 1.7125000000000003e-05, 'epoch': 2.09}\n",
      "{'loss': 0.1775, 'learning_rate': 1.7000000000000003e-05, 'epoch': 2.09}\n",
      "{'loss': 0.1905, 'learning_rate': 1.6875000000000004e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0106, 'learning_rate': 1.675e-05, 'epoch': 2.11}\n",
      "{'loss': 0.0718, 'learning_rate': 1.6625e-05, 'epoch': 2.11}\n",
      "{'loss': 0.038, 'learning_rate': 1.65e-05, 'epoch': 2.12}\n",
      "{'loss': 0.0979, 'learning_rate': 1.6375e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0909, 'learning_rate': 1.6250000000000002e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0671, 'learning_rate': 1.6125000000000002e-05, 'epoch': 2.14}\n",
      "{'loss': 0.1606, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.15}\n",
      "{'loss': 0.1844, 'learning_rate': 1.5875e-05, 'epoch': 2.15}\n",
      "{'loss': 0.1536, 'learning_rate': 1.575e-05, 'epoch': 2.16}\n",
      "{'loss': 0.1668, 'learning_rate': 1.5625e-05, 'epoch': 2.17}\n",
      "{'loss': 0.169, 'learning_rate': 1.55e-05, 'epoch': 2.17}\n",
      "{'loss': 0.1655, 'learning_rate': 1.5375e-05, 'epoch': 2.18}\n",
      "{'loss': 0.2447, 'learning_rate': 1.525e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0055, 'learning_rate': 1.5125e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0416, 'learning_rate': 1.5e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0774, 'learning_rate': 1.4875e-05, 'epoch': 2.21}\n",
      "{'loss': 0.1576, 'learning_rate': 1.475e-05, 'epoch': 2.21}\n",
      "{'loss': 0.0702, 'learning_rate': 1.4625e-05, 'epoch': 2.22}\n",
      "{'loss': 0.1432, 'learning_rate': 1.45e-05, 'epoch': 2.23}\n",
      "{'loss': 0.2557, 'learning_rate': 1.4374999999999999e-05, 'epoch': 2.23}\n",
      "{'loss': 0.1019, 'learning_rate': 1.4249999999999999e-05, 'epoch': 2.24}\n",
      "{'loss': 0.1371, 'learning_rate': 1.4125e-05, 'epoch': 2.25}\n",
      "{'loss': 0.1476, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.25}\n",
      "{'loss': 0.0043, 'learning_rate': 1.3875000000000002e-05, 'epoch': 2.26}\n",
      "{'loss': 0.219, 'learning_rate': 1.3750000000000002e-05, 'epoch': 2.27}\n",
      "{'loss': 0.1248, 'learning_rate': 1.3625e-05, 'epoch': 2.27}\n",
      "{'loss': 0.1915, 'learning_rate': 1.3500000000000001e-05, 'epoch': 2.28}\n",
      "{'loss': 0.0625, 'learning_rate': 1.3375000000000002e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0069, 'learning_rate': 1.3250000000000002e-05, 'epoch': 2.29}\n",
      "{'loss': 0.084, 'learning_rate': 1.3125e-05, 'epoch': 2.3}\n",
      "{'loss': 0.0414, 'learning_rate': 1.3000000000000001e-05, 'epoch': 2.31}\n",
      "{'loss': 0.0876, 'learning_rate': 1.2875000000000001e-05, 'epoch': 2.31}\n",
      "{'loss': 0.1156, 'learning_rate': 1.2750000000000002e-05, 'epoch': 2.32}\n",
      "{'loss': 0.1293, 'learning_rate': 1.2625e-05, 'epoch': 2.33}\n",
      "{'loss': 0.1537, 'learning_rate': 1.25e-05, 'epoch': 2.33}\n",
      "{'loss': 0.0558, 'learning_rate': 1.2375000000000001e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0032, 'learning_rate': 1.225e-05, 'epoch': 2.35}\n",
      "{'loss': 0.0982, 'learning_rate': 1.2125e-05, 'epoch': 2.35}\n",
      "{'loss': 0.1219, 'learning_rate': 1.2e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0606, 'learning_rate': 1.1875e-05, 'epoch': 2.37}\n",
      "{'loss': 0.2591, 'learning_rate': 1.175e-05, 'epoch': 2.37}\n",
      "{'loss': 0.1597, 'learning_rate': 1.1625000000000001e-05, 'epoch': 2.38}\n",
      "{'loss': 0.1519, 'learning_rate': 1.1500000000000002e-05, 'epoch': 2.39}\n",
      "{'loss': 0.0036, 'learning_rate': 1.1375e-05, 'epoch': 2.39}\n",
      "{'loss': 0.3292, 'learning_rate': 1.125e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0112, 'learning_rate': 1.1125000000000001e-05, 'epoch': 2.41}\n",
      "{'loss': 0.1523, 'learning_rate': 1.1000000000000001e-05, 'epoch': 2.41}\n",
      "{'loss': 0.0399, 'learning_rate': 1.0875e-05, 'epoch': 2.42}\n",
      "{'loss': 0.0524, 'learning_rate': 1.075e-05, 'epoch': 2.43}\n",
      "{'loss': 0.1986, 'learning_rate': 1.0625e-05, 'epoch': 2.43}\n",
      "{'loss': 0.0478, 'learning_rate': 1.05e-05, 'epoch': 2.44}\n",
      "{'loss': 0.3105, 'learning_rate': 1.0375e-05, 'epoch': 2.45}\n",
      "{'loss': 0.0235, 'learning_rate': 1.025e-05, 'epoch': 2.45}\n",
      "{'loss': 0.0023, 'learning_rate': 1.0125e-05, 'epoch': 2.46}\n",
      "{'loss': 0.1201, 'learning_rate': 1e-05, 'epoch': 2.47}\n",
      "{'loss': 0.4469, 'learning_rate': 9.875000000000001e-06, 'epoch': 2.47}\n",
      "{'loss': 0.1941, 'learning_rate': 9.750000000000002e-06, 'epoch': 2.48}\n",
      "{'loss': 0.1211, 'learning_rate': 9.625e-06, 'epoch': 2.49}\n",
      "{'loss': 0.1193, 'learning_rate': 9.5e-06, 'epoch': 2.49}\n",
      "{'loss': 0.1027, 'learning_rate': 9.375000000000001e-06, 'epoch': 2.5}\n",
      "{'loss': 0.047, 'learning_rate': 9.25e-06, 'epoch': 2.51}\n",
      "{'loss': 0.1225, 'learning_rate': 9.125e-06, 'epoch': 2.51}\n",
      "{'loss': 0.0441, 'learning_rate': 9e-06, 'epoch': 2.52}\n",
      "{'loss': 0.2174, 'learning_rate': 8.875e-06, 'epoch': 2.53}\n",
      "{'loss': 0.1471, 'learning_rate': 8.75e-06, 'epoch': 2.53}\n",
      "{'loss': 0.0504, 'learning_rate': 8.625e-06, 'epoch': 2.54}\n",
      "{'loss': 0.1619, 'learning_rate': 8.500000000000002e-06, 'epoch': 2.55}\n",
      "{'loss': 0.1678, 'learning_rate': 8.375e-06, 'epoch': 2.55}\n",
      "{'loss': 0.2091, 'learning_rate': 8.25e-06, 'epoch': 2.56}\n",
      "{'loss': 0.1699, 'learning_rate': 8.125000000000001e-06, 'epoch': 2.57}\n",
      "{'loss': 0.2762, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.57}\n",
      "{'loss': 0.0404, 'learning_rate': 7.875e-06, 'epoch': 2.58}\n",
      "{'loss': 0.1059, 'learning_rate': 7.75e-06, 'epoch': 2.59}\n",
      "{'loss': 0.2773, 'learning_rate': 7.625e-06, 'epoch': 2.59}\n",
      "{'loss': 0.1, 'learning_rate': 7.5e-06, 'epoch': 2.6}\n",
      "{'loss': 0.1371, 'learning_rate': 7.375e-06, 'epoch': 2.61}\n",
      "{'loss': 0.2307, 'learning_rate': 7.25e-06, 'epoch': 2.61}\n",
      "{'loss': 0.0821, 'learning_rate': 7.1249999999999995e-06, 'epoch': 2.62}\n",
      "{'loss': 0.0422, 'learning_rate': 7.000000000000001e-06, 'epoch': 2.63}\n",
      "{'loss': 0.0465, 'learning_rate': 6.875000000000001e-06, 'epoch': 2.63}\n",
      "{'loss': 0.0416, 'learning_rate': 6.750000000000001e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0204, 'learning_rate': 6.625000000000001e-06, 'epoch': 2.65}\n",
      "{'loss': 0.2368, 'learning_rate': 6.5000000000000004e-06, 'epoch': 2.65}\n",
      "{'loss': 0.0025, 'learning_rate': 6.375000000000001e-06, 'epoch': 2.66}\n",
      "{'loss': 0.0766, 'learning_rate': 6.25e-06, 'epoch': 2.67}\n",
      "{'loss': 0.0023, 'learning_rate': 6.125e-06, 'epoch': 2.67}\n",
      "{'loss': 0.1087, 'learning_rate': 6e-06, 'epoch': 2.68}\n",
      "{'loss': 0.1691, 'learning_rate': 5.875e-06, 'epoch': 2.69}\n",
      "{'loss': 0.1303, 'learning_rate': 5.750000000000001e-06, 'epoch': 2.69}\n",
      "{'loss': 0.0358, 'learning_rate': 5.625e-06, 'epoch': 2.7}\n",
      "{'loss': 0.0077, 'learning_rate': 5.500000000000001e-06, 'epoch': 2.71}\n",
      "{'loss': 0.1082, 'learning_rate': 5.375e-06, 'epoch': 2.71}\n",
      "{'loss': 0.006, 'learning_rate': 5.25e-06, 'epoch': 2.72}\n",
      "{'loss': 0.1867, 'learning_rate': 5.125e-06, 'epoch': 2.73}\n",
      "{'loss': 0.0018, 'learning_rate': 5e-06, 'epoch': 2.73}\n",
      "{'loss': 0.0641, 'learning_rate': 4.875000000000001e-06, 'epoch': 2.74}\n",
      "{'loss': 0.0805, 'learning_rate': 4.75e-06, 'epoch': 2.75}\n",
      "{'loss': 0.1807, 'learning_rate': 4.625e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0478, 'learning_rate': 4.5e-06, 'epoch': 2.76}\n",
      "{'loss': 0.148, 'learning_rate': 4.375e-06, 'epoch': 2.77}\n",
      "{'loss': 0.0014, 'learning_rate': 4.250000000000001e-06, 'epoch': 2.77}\n",
      "{'loss': 0.0242, 'learning_rate': 4.125e-06, 'epoch': 2.78}\n",
      "{'loss': 0.1682, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.79}\n",
      "{'loss': 0.043, 'learning_rate': 3.875e-06, 'epoch': 2.79}\n",
      "{'loss': 0.0732, 'learning_rate': 3.75e-06, 'epoch': 2.8}\n",
      "{'loss': 0.1298, 'learning_rate': 3.625e-06, 'epoch': 2.81}\n",
      "{'loss': 0.1085, 'learning_rate': 3.5000000000000004e-06, 'epoch': 2.81}\n",
      "{'loss': 0.1268, 'learning_rate': 3.3750000000000003e-06, 'epoch': 2.82}\n",
      "{'loss': 0.065, 'learning_rate': 3.2500000000000002e-06, 'epoch': 2.83}\n",
      "{'loss': 0.0015, 'learning_rate': 3.125e-06, 'epoch': 2.83}\n",
      "{'loss': 0.185, 'learning_rate': 3e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0026, 'learning_rate': 2.8750000000000004e-06, 'epoch': 2.85}\n",
      "{'loss': 0.1735, 'learning_rate': 2.7500000000000004e-06, 'epoch': 2.85}\n",
      "{'loss': 0.1339, 'learning_rate': 2.625e-06, 'epoch': 2.86}\n",
      "{'loss': 0.0794, 'learning_rate': 2.5e-06, 'epoch': 2.87}\n",
      "{'loss': 0.2518, 'learning_rate': 2.375e-06, 'epoch': 2.87}\n",
      "{'loss': 0.0635, 'learning_rate': 2.25e-06, 'epoch': 2.88}\n",
      "{'loss': 0.0838, 'learning_rate': 2.1250000000000004e-06, 'epoch': 2.89}\n",
      "{'loss': 0.1092, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.89}\n",
      "{'loss': 0.0795, 'learning_rate': 1.875e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0406, 'learning_rate': 1.7500000000000002e-06, 'epoch': 2.91}\n",
      "{'loss': 0.0714, 'learning_rate': 1.6250000000000001e-06, 'epoch': 2.91}\n",
      "{'loss': 0.0028, 'learning_rate': 1.5e-06, 'epoch': 2.92}\n",
      "{'loss': 0.0821, 'learning_rate': 1.3750000000000002e-06, 'epoch': 2.93}\n",
      "{'loss': 0.0923, 'learning_rate': 1.25e-06, 'epoch': 2.93}\n",
      "{'loss': 0.1757, 'learning_rate': 1.125e-06, 'epoch': 2.94}\n",
      "{'loss': 0.0014, 'learning_rate': 1.0000000000000002e-06, 'epoch': 2.95}\n",
      "{'loss': 0.1198, 'learning_rate': 8.750000000000001e-07, 'epoch': 2.95}\n",
      "{'loss': 0.1771, 'learning_rate': 7.5e-07, 'epoch': 2.96}\n",
      "{'loss': 0.0781, 'learning_rate': 6.25e-07, 'epoch': 2.97}\n",
      "{'loss': 0.0756, 'learning_rate': 5.000000000000001e-07, 'epoch': 2.97}\n",
      "{'loss': 0.0686, 'learning_rate': 3.75e-07, 'epoch': 2.98}\n",
      "{'loss': 0.078, 'learning_rate': 2.5000000000000004e-07, 'epoch': 2.99}\n",
      "{'loss': 0.1424, 'learning_rate': 1.2500000000000002e-07, 'epoch': 2.99}\n",
      "{'loss': 0.0721, 'learning_rate': 0.0, 'epoch': 3.0}\n",
      "{'train_runtime': 5230.8788, 'train_samples_per_second': 6.882, 'train_steps_per_second': 0.86, 'train_loss': 0.2703438252421717, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e6621f2c7246ac9ebf7da849887719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid text_input type <class 'list'> (required str or OrderedDict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     augmented_texts \u001b[38;5;241m=\u001b[39m augmenter\u001b[38;5;241m.\u001b[39maugment(examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: augmented_texts}\n\u001b[0;32m---> 52\u001b[0m dataset_augmented \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugment_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m dataset_augmented \u001b[38;5;241m=\u001b[39m dataset_augmented\u001b[38;5;241m.\u001b[39mmap(encode, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m dataset_augmented\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/venv/jae-dev/lib/python3.11/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/jae-dev/lib/python3.11/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/jae-dev/lib/python3.11/site-packages/datasets/arrow_dataset.py:3105\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3101\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3102\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3103\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3104\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3105\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3106\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3107\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m~/venv/jae-dev/lib/python3.11/site-packages/datasets/arrow_dataset.py:3482\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3478\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3479\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3480\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3482\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3490\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3491\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/jae-dev/lib/python3.11/site-packages/datasets/arrow_dataset.py:3361\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3360\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3361\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3363\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3364\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3365\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m, in \u001b[0;36maugment_data\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maugment_data\u001b[39m(examples):\n\u001b[0;32m---> 49\u001b[0m     augmented_texts \u001b[38;5;241m=\u001b[39m \u001b[43maugmenter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: augmented_texts}\n",
      "File \u001b[0;32m~/venv/jae-dev/lib/python3.11/site-packages/textattack/augmentation/augmenter.py:114\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maugment\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns all possible augmentations of ``text`` according to\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    ``self.transformation``.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     attacked_text \u001b[38;5;241m=\u001b[39m \u001b[43mAttackedText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     original_text \u001b[38;5;241m=\u001b[39m attacked_text\n\u001b[1;32m    116\u001b[0m     all_transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venv/jae-dev/lib/python3.11/site-packages/textattack/shared/attacked_text.py:54\u001b[0m, in \u001b[0;36mAttackedText.__init__\u001b[0;34m(self, text_input, attack_attrs)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_input \u001b[38;5;241m=\u001b[39m text_input\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid text_input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(text_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (required str or OrderedDict)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Process input lazily.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid text_input type <class 'list'> (required str or OrderedDict)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from textattack.augmentation import EmbeddingAugmenter\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"ag_news\", split='train[:10%]')  # Using a subset for quick demonstration\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "\n",
    "# Function to encode the dataset\n",
    "def encode(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "dataset_encoded = dataset.map(encode, batched=True)\n",
    "dataset_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size for training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Standard Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_encoded,\n",
    "    eval_dataset=dataset_encoded,\n",
    ")\n",
    "\n",
    "# Train the standard model\n",
    "trainer.train()\n",
    "\n",
    "# Augmenter for adversarial training\n",
    "augmenter = EmbeddingAugmenter()\n",
    "\n",
    "# Augment training data\n",
    "def augment_data(examples):\n",
    "    augmented_texts = augmenter.augment(examples['text'])\n",
    "    return {'text': augmented_texts}\n",
    "\n",
    "dataset_augmented = dataset.map(augment_data, batched=True)\n",
    "dataset_augmented = dataset_augmented.map(encode, batched=True)\n",
    "dataset_augmented.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Train model with augmented data\n",
    "trainer_with_augmentation = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_augmented,\n",
    "    eval_dataset=dataset_encoded,\n",
    ")\n",
    "\n",
    "trainer_with_augmentation.train()\n",
    "\n",
    "# Evaluate models\n",
    "eval_standard = trainer.evaluate()\n",
    "eval_augmented = trainer_with_augmentation.evaluate()\n",
    "\n",
    "print(\"Standard Training Evaluation:\", eval_standard)\n",
    "print(\"Adversarial Training Evaluation:\", eval_augmented)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jae-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
