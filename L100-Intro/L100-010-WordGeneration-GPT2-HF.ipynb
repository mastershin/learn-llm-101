{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"R8cH3URSjNn1"},"outputs":[],"source":["%pip install pandas torch transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5059,"status":"ok","timestamp":1713058679542,"user":{"displayName":"Jae Shin","userId":"06683230645136354404"},"user_tz":420},"id":"yx7EzsSHjQFC","outputId":"f3792b53-fed5-4101-d7a2-1a68f295c22a"},"outputs":[{"name":"stdout","output_type":"stream","text":["model_id: gpt2\n","Total tokens in tokenizer: 50257\n","text: The fox is jumping over the\n","input_ids: tensor([[  464, 21831,   318, 14284,   625,   262]])\n","id and word: [(464, 'The'), (21831, ' fox'), (318, ' is'), (14284, ' jumping'), (625, ' over'), (262, ' the')]\n","prob.shape torch.Size([50257])\n","          id    token      prob\n","13990  13990    fence  0.106365\n","5743    5743     edge  0.027167\n","5509    5509     tree  0.026133\n","3355    3355     wall  0.023594\n","1353    1353      top  0.014355\n","7150    7150    trees  0.013775\n","19516  19516    cliff  0.012385\n","2046    2046     fire  0.009482\n","37413  37413   bushes  0.009091\n","2318    2318      bar  0.009064\n","Next most probable token id: 13990\n","Next probable token:  fence\n","New sentence: The fox is jumping over the fence\n"]}],"source":["# Purpose: To learn how words / tokens are used in LLMs.\n","# Using a small language model like gpt2, pythia smaller models,\n","# Experiment with sentences and see how next words are predicted\n","#\n","# Given a text, calculate next word probabilities and display prob and word.\n","# Also, generate longer sentences from original text.\n","\n","import torch\n","import pandas as pd\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model_id = \"gpt2\"\n","# model_id = \"EleutherAI/pythia-14m\"\n","\n","original_text = \"The fox is jumping over the\"\n","# original_text = \"The prince is in love with a\"\n","# original_text = \"The man is marrying with a\"\n","text = original_text\n","print('model_id:', model_id)\n","\n","model = AutoModelForCausalLM.from_pretrained(model_id)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","total_tokens = len(tokenizer)\n","print(f\"Total tokens in tokenizer: {total_tokens}\")\n","\n","# partial sentence\n","inputs = tokenizer(text, return_tensors=\"pt\")\n","# inputs: {'input_ids': tensor([[  464, 21831,   318, 14284,   625,   262]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n","\n","print('text:', text)\n","print('input_ids:', inputs[\"input_ids\"])\n","print('id and word:', [(id.item(), tokenizer.decode(id)) for id in inputs[\"input_ids\"][0]])\n","\n","# Calculate probabilities for the next words (token)\n","def get_next_word_probs(model, inputs):\n","  with torch.no_grad():\n","    # logits = model(**inputs).logits[:, -1, :]\n","    output = model(**inputs)  # CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[...]]])\n","    logits = output.logits # torch.Size([1, 6, 50257])\n","    # print('output.logits.shape', logits.shape)\n","    last_columns = logits[:, -1, :]  # torch.Size([1, 50257])\n","    # print('last_columns.shape', last_columns.shape)\n","\n","    prob = torch.nn.functional.softmax(last_columns[0], dim=-1)\n","\n","  return prob\n","\n","def show_next_token_choices(prob, top_n=5):\n","  return pd.DataFrame(\n","      [\n","          (id, tokenizer.decode(id), p.item())\n","          for id, p in enumerate(prob)\n","          if p.item()\n","      ],\n","      columns=[\"id\", \"token\", \"prob\"],\n","  ).sort_values(\"prob\", ascending=False)[:top_n]\n","\n","prob = get_next_word_probs(model, inputs)\n","print('prob.shape', prob.shape)  # torch.Size([50257])\n","print(show_next_token_choices(prob, 10))\n","\n","next_token_id = torch.argmax(prob).item()\n","next_word = tokenizer.decode(next_token_id)\n","print(f\"Next most probable token id: {next_token_id}\")\n","print(f\"Next probable token: {next_word}\")\n","text += next_word\n","print(f\"New sentence: {text}\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3573,"status":"ok","timestamp":1713058770844,"user":{"displayName":"Jae Shin","userId":"06683230645136354404"},"user_tz":420},"id":"AUsn-qcBLQNH","outputId":"10643eab-95d8-497e-9d02-c27ed01c459e"},"outputs":[{"name":"stdout","output_type":"stream","text":["The fox is jumping over the fence.\n","\n","\n","\"I'm going to kill you!\"\n","\n","\n","\"I'm going to"]}],"source":["# Generate next words with a for-loop to see in detail (slower, streaming)\n","text = original_text\n","inputs = tokenizer(original_text, return_tensors=\"pt\")\n","print(text, end='')\n","\n","for i in range(20):\n","  inputs = tokenizer(text, return_tensors=\"pt\")\n","  prob = get_next_word_probs(model, inputs)\n","  next_token_id = torch.argmax(prob).item()\n","  next_word = tokenizer.decode(next_token_id)\n","  print(next_word, end='')\n","  text += next_word"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8883,"status":"ok","timestamp":1713059058388,"user":{"displayName":"Jae Shin","userId":"06683230645136354404"},"user_tz":420},"id":"YM5GsHFROs2h","outputId":"0292479c-ee47-47f8-fe33-e45f98aa83ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["The fox is jumping over the fence.\n","\n","\"I don't know what's going on, but I think he's going to jump over the fence,\" he said. \"He's going to jump over the fence. He's going to jump\n"]}],"source":["# Generate entire text, with a given maximum token length (faster)\n","# The sentence is likely different each time, due to temperature (randomness)\n","# Or even repeating same sentence (repetition_penalty controls this)\n","inputs = tokenizer(original_text, return_tensors=\"pt\")\n","output = model.generate(**inputs, max_length=50, pad_token_id=tokenizer.eos_token_id,\n","    do_sample=True,  # Enable sampling for temperature\n","    temperature=0.9,\n","    repetition_penalty=1.5,\n","    num_beams=5  # Number of beams for beam search (optional)\n","    )\n","decoded = tokenizer.decode(output[0])\n","print(decoded)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMai6w+XXtiXGXaoAMNCA1P","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
